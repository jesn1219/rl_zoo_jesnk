{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/env2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import importlib\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import yaml\n",
    "from huggingface_sb3 import EnvironmentName\n",
    "from stable_baselines3.common.callbacks import tqdm\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "\n",
    "import rl_zoo3.import_envs  # noqa: F401 pylint: disable=unused-import\n",
    "from rl_zoo3 import ALGOS, create_test_env, get_saved_hyperparams\n",
    "from rl_zoo3.exp_manager import ExperimentManager\n",
    "from rl_zoo3.load_from_hub import download_from_hub\n",
    "from rl_zoo3.utils import StoreDict, get_model_path\n",
    "import cv2\n",
    "\n",
    "def enjoy(args) -> None:  # noqa: C901\n",
    "    # Going through custom gym packages to let them register in the global registory\n",
    "    for env_module in args.gym_packages:\n",
    "        importlib.import_module(env_module)\n",
    "\n",
    "    env_name: EnvironmentName = args.env\n",
    "    algo = args.algo\n",
    "    folder = args.folder\n",
    "\n",
    "    try:\n",
    "        _, model_path, log_path = get_model_path(\n",
    "            args.exp_id,\n",
    "            folder,\n",
    "            algo,\n",
    "            env_name,\n",
    "            args.load_best,\n",
    "            args.load_checkpoint,\n",
    "            args.load_last_checkpoint,\n",
    "        )\n",
    "    except (AssertionError, ValueError) as e:\n",
    "        # Special case for rl-trained agents\n",
    "        # auto-download from the hub\n",
    "        if \"rl-trained-agents\" not in folder:\n",
    "            raise e\n",
    "        else:\n",
    "            print(\"Pretrained model not found, trying to download it from sb3 Huggingface hub: https://huggingface.co/sb3\")\n",
    "            # Auto-download\n",
    "            download_from_hub(\n",
    "                algo=algo,\n",
    "                env_name=env_name,\n",
    "                exp_id=args.exp_id,\n",
    "                folder=folder,\n",
    "                organization=\"sb3\",\n",
    "                repo_name=None,\n",
    "                force=False,\n",
    "            )\n",
    "            # Try again\n",
    "            _, model_path, log_path = get_model_path(\n",
    "                args.exp_id,\n",
    "                folder,\n",
    "                algo,\n",
    "                env_name,\n",
    "                args.load_best,\n",
    "                args.load_checkpoint,\n",
    "                args.load_last_checkpoint,\n",
    "            )\n",
    "\n",
    "    print(f\"Loading {model_path}\")\n",
    "\n",
    "    # Off-policy algorithm only support one env for now\n",
    "    off_policy_algos = [\"qrdqn\", \"dqn\", \"ddpg\", \"sac\", \"her\", \"td3\", \"tqc\"]\n",
    "\n",
    "    if algo in off_policy_algos:\n",
    "        args.n_envs = 1\n",
    "\n",
    "    set_random_seed(args.seed)\n",
    "\n",
    "    if args.num_threads > 0:\n",
    "        if args.verbose > 1:\n",
    "            print(f\"Setting torch.num_threads to {args.num_threads}\")\n",
    "        th.set_num_threads(args.num_threads)\n",
    "\n",
    "    is_atari = ExperimentManager.is_atari(env_name.gym_id)\n",
    "    is_minigrid = ExperimentManager.is_minigrid(env_name.gym_id)\n",
    "\n",
    "    stats_path = os.path.join(log_path, env_name)\n",
    "    hyperparams, maybe_stats_path = get_saved_hyperparams(stats_path, norm_reward=args.norm_reward, test_mode=True)\n",
    "\n",
    "    # load env_kwargs if existing\n",
    "    env_kwargs = {}\n",
    "    args_path = os.path.join(log_path, env_name, \"args.yml\")\n",
    "    if os.path.isfile(args_path):\n",
    "        with open(args_path) as f:\n",
    "            loaded_args = yaml.load(f, Loader=yaml.UnsafeLoader)  # pytype: disable=module-attr\n",
    "            if loaded_args[\"env_kwargs\"] is not None:\n",
    "                env_kwargs = loaded_args[\"env_kwargs\"]\n",
    "    # overwrite with command line arguments\n",
    "    if args.env_kwargs is not None:\n",
    "        env_kwargs.update(args.env_kwargs)\n",
    "\n",
    "    log_dir = args.reward_log if args.reward_log != \"\" else None\n",
    "\n",
    "    env = create_test_env(\n",
    "        env_name.gym_id,\n",
    "        n_envs=args.n_envs,\n",
    "        stats_path=maybe_stats_path,\n",
    "        seed=args.seed,\n",
    "        log_dir=log_dir,\n",
    "        should_render=not args.no_render if not args.render_rgb else False, # jskang \n",
    "        hyperparams=hyperparams,\n",
    "        env_kwargs=env_kwargs,\n",
    "    )\n",
    "\n",
    "    kwargs = dict(seed=args.seed)\n",
    "    if algo in off_policy_algos:\n",
    "        # Dummy buffer size as we don't need memory to enjoy the trained agent\n",
    "        kwargs.update(dict(buffer_size=1))\n",
    "        # Hack due to breaking change in v1.6\n",
    "        # handle_timeout_termination cannot be at the same time\n",
    "        # with optimize_memory_usage\n",
    "        if \"optimize_memory_usage\" in hyperparams:\n",
    "            kwargs.update(optimize_memory_usage=False)\n",
    "\n",
    "    # Check if we are running python 3.8+\n",
    "    # we need to patch saved model under python 3.6/3.7 to load them\n",
    "    newer_python_version = sys.version_info.major == 3 and sys.version_info.minor >= 8\n",
    "\n",
    "    custom_objects = {}\n",
    "    if newer_python_version or args.custom_objects:\n",
    "        custom_objects = {\n",
    "            \"learning_rate\": 0.0,\n",
    "            \"lr_schedule\": lambda _: 0.0,\n",
    "            \"clip_range\": lambda _: 0.0,\n",
    "        }\n",
    "\n",
    "    if \"HerReplayBuffer\" in hyperparams.get(\"replay_buffer_class\", \"\"):\n",
    "        kwargs[\"env\"] = env\n",
    "\n",
    "    model = ALGOS[algo].load(model_path, custom_objects=custom_objects, device=args.device, **kwargs)\n",
    "    #print(env.render_mode)\n",
    "    #env.render_mode = 'rgb_array'\n",
    "    #print(env.render_mode)\n",
    "\n",
    "    obs = env.reset()\n",
    "\n",
    "    # Deterministic by default except for atari games\n",
    "    stochastic = args.stochastic or (is_atari or is_minigrid) and not args.deterministic\n",
    "    deterministic = not stochastic\n",
    "\n",
    "    episode_reward = 0.0\n",
    "    episode_rewards, episode_lengths = [], []\n",
    "    ep_len = 0\n",
    "    # For HER, monitor success rate\n",
    "    successes = []\n",
    "    lstm_states = None\n",
    "    episode_start = np.ones((env.num_envs,), dtype=bool)\n",
    "\n",
    "    generator = range(args.n_timesteps)\n",
    "    if args.progress:\n",
    "        if tqdm is None:\n",
    "            raise ImportError(\"Please install tqdm and rich to use the progress bar\")\n",
    "        generator = tqdm(generator)\n",
    "\n",
    "    try:\n",
    "        frames = []\n",
    "        import mediapy as media\n",
    "        for _ in generator:\n",
    "            action, lstm_states = model.predict(\n",
    "                obs,  # type: ignore[arg-type]\n",
    "                state=lstm_states,\n",
    "                episode_start=episode_start,\n",
    "                deterministic=deterministic,\n",
    "            )\n",
    "\n",
    "            obs, reward, done, infos = env.step(action)\n",
    "\n",
    "            episode_start = done\n",
    "\n",
    "            if not args.no_render:\n",
    "                #env.render(\"human\")\n",
    "                pass\n",
    "            if args.render_rgb :\n",
    "                frame = env.render('rgb_array')\n",
    "                #frame = env.render()\n",
    "                # add episode number\n",
    "                frame = cv2.putText(frame, f\"episode: {_}\", (10, 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "                frame = cv2.putText(frame, f\"reward: {reward[0]:.2f}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "                frame = cv2.putText(frame, f\"done: {done}\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "                frame = cv2.putText(frame, f\"episode_reward: {episode_reward:.2f}\", (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "                frame = cv2.putText(frame, f\"episode_length: {ep_len:.2f}\", (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "                if infos is not None:\n",
    "                    frame = cv2.putText(frame, f\"infos: {infos[0]}\", (10, 110), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "                frames.append(frame)\n",
    "                #print(type(model))\n",
    "\n",
    "            episode_reward += reward[0]\n",
    "            ep_len += 1\n",
    "\n",
    "            if args.n_envs == 1:\n",
    "                # For atari the return reward is not the atari score\n",
    "                # so we have to get it from the infos dict\n",
    "                if is_atari and infos is not None and args.verbose >= 1:\n",
    "                    episode_infos = infos[0].get(\"episode\")\n",
    "                    if episode_infos is not None:\n",
    "                        print(f\"Atari Episode Score: {episode_infos['r']:.2f}\")\n",
    "                        print(\"Atari Episode Length\", episode_infos[\"l\"])\n",
    "\n",
    "                if done and not is_atari and args.verbose > 0:\n",
    "                    # NOTE: for env using VecNormalize, the mean reward\n",
    "                    # is a normalized reward when `--norm_reward` flag is passed\n",
    "                    print(f\"Episode Reward: {episode_reward:.2f}\")\n",
    "                    print(\"Episode Length\", ep_len)\n",
    "                    episode_rewards.append(episode_reward)\n",
    "                    episode_lengths.append(ep_len)\n",
    "                    episode_reward = 0.0\n",
    "                    ep_len = 0\n",
    "\n",
    "                # Reset also when the goal is achieved when using HER\n",
    "                if done and infos[0].get(\"is_success\") is not None:\n",
    "                    if args.verbose > 1:\n",
    "                        print(\"Success?\", infos[0].get(\"is_success\", False))\n",
    "\n",
    "                    if infos[0].get(\"is_success\") is not None:\n",
    "                        successes.append(infos[0].get(\"is_success\", False))\n",
    "                        episode_reward, ep_len = 0.0, 0\n",
    "                \n",
    "                        \n",
    "                \n",
    "        media.show_video(frames, fps=30)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    if args.verbose > 0 and len(successes) > 0:\n",
    "        print(f\"Success rate: {100 * np.mean(successes):.2f}%\")\n",
    "\n",
    "    if args.verbose > 0 and len(episode_rewards) > 0:\n",
    "        print(f\"{len(episode_rewards)} Episodes\")\n",
    "        print(f\"Mean reward: {np.mean(episode_rewards):.2f} +/- {np.std(episode_rewards):.2f}\")\n",
    "\n",
    "    if args.verbose > 0 and len(episode_lengths) > 0:\n",
    "        print(f\"Mean episode length: {np.mean(episode_lengths):.2f} +/- {np.std(episode_lengths):.2f}\")\n",
    "\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 22 04:23:13 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.147.05   Driver Version: 525.147.05   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\n",
      "|  0%   50C    P8    31W / 175W |     94MiB /  8192MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n",
      "Setting environment variable to use GPU rendering:\n",
      "env: MUJOCO_GL=egl\n",
      "Checking that the installation succeeded:\n",
      "Installation successful.\n",
      "Installing mediapy:\n"
     ]
    }
   ],
   "source": [
    "#@title Check if installation was successful\n",
    "\n",
    "#from google.colab import files\n",
    "\n",
    "import distutils.util\n",
    "import os\n",
    "import subprocess\n",
    "if subprocess.run('nvidia-smi').returncode:\n",
    "  raise RuntimeError(\n",
    "      'Cannot communicate with GPU. '\n",
    "      'Make sure you are using a GPU Colab runtime. '\n",
    "      'Go to the Runtime menu and select Choose runtime type.')\n",
    "\n",
    "# Add an ICD config so that glvnd can pick up the Nvidia EGL driver.\n",
    "# This is usually installed as part of an Nvidia driver package, but the Colab\n",
    "# kernel doesn't install its driver via APT, and as a result the ICD is missing.\n",
    "# (https://github.com/NVIDIA/libglvnd/blob/master/src/EGL/icd_enumeration.md)\n",
    "NVIDIA_ICD_CONFIG_PATH = '/usr/share/glvnd/egl_vendor.d/10_nvidia.json'\n",
    "if not os.path.exists(NVIDIA_ICD_CONFIG_PATH):\n",
    "  with open(NVIDIA_ICD_CONFIG_PATH, 'w') as f:\n",
    "    f.write(\"\"\"{\n",
    "    \"file_format_version\" : \"1.0.0\",\n",
    "    \"ICD\" : {\n",
    "        \"library_path\" : \"libEGL_nvidia.so.0\"\n",
    "    }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "# Configure MuJoCo to use the EGL rendering backend (requires GPU)\n",
    "print('Setting environment variable to use GPU rendering:')\n",
    "%env MUJOCO_GL=egl\n",
    "\n",
    "try:\n",
    "  print('Checking that the installation succeeded:')\n",
    "  import mujoco\n",
    "  mujoco.MjModel.from_xml_string('<mujoco/>')\n",
    "except Exception as e:\n",
    "  raise e from RuntimeError(\n",
    "      'Something went wrong during installation. Check the shell output above '\n",
    "      'for more information.\\n'\n",
    "      'If using a hosted Colab runtime, make sure you enable GPU acceleration '\n",
    "      'by going to the Runtime menu and selecting \"Choose runtime type\".')\n",
    "\n",
    "print('Installation successful.')\n",
    "\n",
    "#@title Import packages for plotting and creating graphics\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "from typing import Callable, NamedTuple, Optional, Union, List\n",
    "\n",
    "# Graphics and plotting.\n",
    "print('Installing mediapy:')\n",
    "!command -v ffmpeg >/dev/null || (apt update && apt install -y ffmpeg)\n",
    "!pip install -q mediapy\n",
    "import mediapy as media\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# More legible printing from numpy.\n",
    "np.set_printoptions(precision=3, suppress=True, linewidth=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DISPLAY=897985abde1b:10.0\n"
     ]
    }
   ],
   "source": [
    "%env DISPLAY=897985abde1b:10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'\\nimport random\\nimport gymnasium as gym\\nenv = gym.make('Hopper', render_mode='rgb_array')\\nenv.reset()\\nn_frames = 120\\nheight = 480\\nwidth = 480\\nframes = []\\n#import mujoco\\nmodel = env.model\\ndata = env.data\\n\\nmujoco.mj_step(model, data)\\nrenderer = mujoco.Renderer(model, height, width)\\nmujoco.mj_resetDataKeyframe(model, data,0)\\n\\njoint_names = ['thigh_joint', 'leg_joint', 'foot_joint']\\njoint_ranges = [model.joint(joint).range for joint in joint_names]\\nfor i in range(n_frames):\\n  while data.time < i/30.0:   \\n    new_action = []\\n        \\n    # ob, reward, terminated, False, {}\\n    obs, reward, terminated, _, _ = env.step(new_action)\\n    #mujoco.mj_step(model, data)\\n  renderer.update_scene(data)\\n  frame = renderer.render()\\n  frames.append(frame)\\nmedia.show_video(frames, fps=30)\\n\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''''\n",
    "import random\n",
    "import gymnasium as gym\n",
    "env = gym.make('Hopper', render_mode='rgb_array')\n",
    "env.reset()\n",
    "n_frames = 120\n",
    "height = 480\n",
    "width = 480\n",
    "frames = []\n",
    "#import mujoco\n",
    "model = env.model\n",
    "data = env.data\n",
    "\n",
    "mujoco.mj_step(model, data)\n",
    "renderer = mujoco.Renderer(model, height, width)\n",
    "mujoco.mj_resetDataKeyframe(model, data,0)\n",
    "\n",
    "joint_names = ['thigh_joint', 'leg_joint', 'foot_joint']\n",
    "joint_ranges = [model.joint(joint).range for joint in joint_names]\n",
    "for i in range(n_frames):\n",
    "  while data.time < i/30.0:   \n",
    "    new_action = []\n",
    "        \n",
    "    # ob, reward, terminated, False, {}\n",
    "    obs, reward, terminated, _, _ = env.step(new_action)\n",
    "    #mujoco.mj_step(model, data)\n",
    "  renderer.update_scene(data)\n",
    "  frame = renderer.render()\n",
    "  frames.append(frame)\n",
    "media.show_video(frames, fps=30)\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading latest experiment, id=9\n",
      "Loading ./logs/tqc/PointMaze_Large-v3_9/rl_model_1000000_steps.zip\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Expecting a torch Tensor, but got <class 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/research/rl_zoo_jesnk/enjoy.ipynb Cell 5\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfisher-docker/research/rl_zoo_jesnk/enjoy.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=67'>68</a>\u001b[0m args\u001b[39m.\u001b[39menv_kwargs \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mrender_mode\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mrgb_array\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mcontinuing_task\u001b[39m\u001b[39m'\u001b[39m:\u001b[39mFalse\u001b[39;00m}\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfisher-docker/research/rl_zoo_jesnk/enjoy.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m args\u001b[39m.\u001b[39mn_timesteps \u001b[39m=\u001b[39m \u001b[39m3000\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bfisher-docker/research/rl_zoo_jesnk/enjoy.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=69'>70</a>\u001b[0m enjoy(args)\n",
      "\u001b[1;32m/research/rl_zoo_jesnk/enjoy.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bfisher-docker/research/rl_zoo_jesnk/enjoy.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=191'>192</a>\u001b[0m     frames\u001b[39m.\u001b[39mappend(frame)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bfisher-docker/research/rl_zoo_jesnk/enjoy.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=192'>193</a>\u001b[0m     \u001b[39m#print(type(model))\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bfisher-docker/research/rl_zoo_jesnk/enjoy.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=193'>194</a>\u001b[0m     \u001b[39mprint\u001b[39m(model\u001b[39m.\u001b[39;49mcritic(obs,action))\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bfisher-docker/research/rl_zoo_jesnk/enjoy.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=195'>196</a>\u001b[0m episode_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bfisher-docker/research/rl_zoo_jesnk/enjoy.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=196'>197</a>\u001b[0m ep_len \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/env2/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/env2/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/env2/lib/python3.10/site-packages/sb3_contrib/tqc/policies.py:238\u001b[0m, in \u001b[0;36mCritic.forward\u001b[0;34m(self, obs, action)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, obs: PyTorchObs, action: th\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m th\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m    235\u001b[0m     \u001b[39m# Learn the features extractor using the policy loss only\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[39m# when the features_extractor is shared with the actor\u001b[39;00m\n\u001b[1;32m    237\u001b[0m     \u001b[39mwith\u001b[39;00m th\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshare_features_extractor):\n\u001b[0;32m--> 238\u001b[0m         features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mextract_features(obs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures_extractor)\n\u001b[1;32m    239\u001b[0m     qvalue_input \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39mcat([features, action], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    240\u001b[0m     quantiles \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39mstack(\u001b[39mtuple\u001b[39m(qf(qvalue_input) \u001b[39mfor\u001b[39;00m qf \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_networks), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/env2/lib/python3.10/site-packages/stable_baselines3/common/policies.py:130\u001b[0m, in \u001b[0;36mBaseModel.extract_features\u001b[0;34m(self, obs, features_extractor)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_features\u001b[39m(\u001b[39mself\u001b[39m, obs: PyTorchObs, features_extractor: BaseFeaturesExtractor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m th\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m    123\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m    Preprocess the observation if needed and extract features.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39m    :return: The extracted features\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m     preprocessed_obs \u001b[39m=\u001b[39m preprocess_obs(obs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobservation_space, normalize_images\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnormalize_images)\n\u001b[1;32m    131\u001b[0m     \u001b[39mreturn\u001b[39;00m features_extractor(preprocessed_obs)\n",
      "File \u001b[0;32m/env2/lib/python3.10/site-packages/stable_baselines3/common/preprocessing.py:113\u001b[0m, in \u001b[0;36mpreprocess_obs\u001b[0;34m(obs, observation_space, normalize_images)\u001b[0m\n\u001b[1;32m    111\u001b[0m     preprocessed_obs \u001b[39m=\u001b[39m {}\n\u001b[1;32m    112\u001b[0m     \u001b[39mfor\u001b[39;00m key, _obs \u001b[39min\u001b[39;00m obs\u001b[39m.\u001b[39mitems():\n\u001b[0;32m--> 113\u001b[0m         preprocessed_obs[key] \u001b[39m=\u001b[39m preprocess_obs(_obs, observation_space[key], normalize_images\u001b[39m=\u001b[39;49mnormalize_images)\n\u001b[1;32m    114\u001b[0m     \u001b[39mreturn\u001b[39;00m preprocessed_obs  \u001b[39m# type: ignore[return-value]\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(obs, th\u001b[39m.\u001b[39mTensor), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpecting a torch Tensor, but got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(obs)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m/env2/lib/python3.10/site-packages/stable_baselines3/common/preprocessing.py:116\u001b[0m, in \u001b[0;36mpreprocess_obs\u001b[0;34m(obs, observation_space, normalize_images)\u001b[0m\n\u001b[1;32m    113\u001b[0m         preprocessed_obs[key] \u001b[39m=\u001b[39m preprocess_obs(_obs, observation_space[key], normalize_images\u001b[39m=\u001b[39mnormalize_images)\n\u001b[1;32m    114\u001b[0m     \u001b[39mreturn\u001b[39;00m preprocessed_obs  \u001b[39m# type: ignore[return-value]\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(obs, th\u001b[39m.\u001b[39mTensor), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpecting a torch Tensor, but got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(obs)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(observation_space, spaces\u001b[39m.\u001b[39mBox):\n\u001b[1;32m    119\u001b[0m     \u001b[39mif\u001b[39;00m normalize_images \u001b[39mand\u001b[39;00m is_image_space(observation_space):\n",
      "\u001b[0;31mAssertionError\u001b[0m: Expecting a torch Tensor, but got <class 'numpy.ndarray'>"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--env\", help=\"environment ID\", type=EnvironmentName, default=\"CartPole-v1\")\n",
    "parser.add_argument(\"-f\", \"--folder\", help=\"Log folder\", type=str, default=\"rl-trained-agents\")\n",
    "parser.add_argument(\"--algo\", help=\"RL Algorithm\", default=\"ppo\", type=str, required=False, choices=list(ALGOS.keys()))\n",
    "parser.add_argument(\"-n\", \"--n-timesteps\", help=\"number of timesteps\", default=1000, type=int)\n",
    "parser.add_argument(\"--num-threads\", help=\"Number of threads for PyTorch (-1 to use default)\", default=-1, type=int)\n",
    "parser.add_argument(\"--n-envs\", help=\"number of environments\", default=1, type=int)\n",
    "parser.add_argument(\"--exp-id\", help=\"Experiment ID (default: 0: latest, -1: no exp folder)\", default=0, type=int)\n",
    "parser.add_argument(\"--verbose\", help=\"Verbose mode (0: no output, 1: INFO)\", default=1, type=int)\n",
    "parser.add_argument(\n",
    "    \"--no-render\", action=\"store_true\", default=False, help=\"Do not render the environment (useful for tests)\"\n",
    ")\n",
    "parser.add_argument(\"--deterministic\", action=\"store_true\", default=False, help=\"Use deterministic actions\")\n",
    "parser.add_argument(\"--device\", help=\"PyTorch device to be use (ex: cpu, cuda...)\", default=\"auto\", type=str)\n",
    "parser.add_argument(\n",
    "    \"--load-best\", action=\"store_true\", default=False, help=\"Load best model instead of last model if available\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--load-checkpoint\",\n",
    "    type=int,\n",
    "    help=\"Load checkpoint instead of last model if available, \"\n",
    "    \"you must pass the number of timesteps corresponding to it\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--load-last-checkpoint\",\n",
    "    action=\"store_true\",\n",
    "    default=False,\n",
    "    help=\"Load last checkpoint instead of last model if available\",\n",
    ")\n",
    "parser.add_argument(\"--stochastic\", action=\"store_true\", default=False, help=\"Use stochastic actions\")\n",
    "parser.add_argument(\n",
    "    \"--norm-reward\", action=\"store_true\", default=False, help=\"Normalize reward if applicable (trained with VecNormalize)\"\n",
    ")\n",
    "parser.add_argument(\"--seed\", help=\"Random generator seed\", type=int, default=0)\n",
    "parser.add_argument(\"--reward-log\", help=\"Where to log reward\", default=\"\", type=str)\n",
    "parser.add_argument(\n",
    "    \"--gym-packages\",\n",
    "    type=str,\n",
    "    nargs=\"+\",\n",
    "    default=[],\n",
    "    help=\"Additional external Gym environment package modules to import\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--env-kwargs\", type=str, nargs=\"+\", action=StoreDict, help=\"Optional keyword argument to pass to the env constructor\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--custom-objects\", action=\"store_true\", default=False, help=\"Use custom objects to solve loading issues\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"-P\",\n",
    "    \"--progress\",\n",
    "    action=\"store_true\",\n",
    "    default=False,\n",
    "    help=\"if toggled, display a progress bar using tqdm and rich\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--render_rgb\", action='store_true', default=False, help=\"if toggled, render rgb array\"\n",
    ")\n",
    "#args = parser.parse_args()\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "args.env = EnvironmentName('PointMaze_Large-v3')\n",
    "args.algo = 'tqc'\n",
    "args.load_last_checkpoint = True\n",
    "args.no_render = True\n",
    "args.render_rgb = True\n",
    "args.folder = './logs/'\n",
    "args.env_kwargs = {'render_mode': 'rgb_array','continuing_task':False}\n",
    "args.n_timesteps = 3000\n",
    "enjoy(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

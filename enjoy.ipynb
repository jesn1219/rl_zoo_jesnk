{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/env2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import importlib\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import yaml\n",
    "from huggingface_sb3 import EnvironmentName\n",
    "from stable_baselines3.common.callbacks import tqdm\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "\n",
    "import rl_zoo3.import_envs  # noqa: F401 pylint: disable=unused-import\n",
    "from rl_zoo3 import ALGOS, create_test_env, get_saved_hyperparams\n",
    "from rl_zoo3.exp_manager import ExperimentManager\n",
    "from rl_zoo3.load_from_hub import download_from_hub\n",
    "from rl_zoo3.utils import StoreDict, get_model_path\n",
    "\n",
    "\n",
    "def enjoy(args) -> None:  # noqa: C901\n",
    "    # Going through custom gym packages to let them register in the global registory\n",
    "    for env_module in args.gym_packages:\n",
    "        importlib.import_module(env_module)\n",
    "\n",
    "    env_name: EnvironmentName = args.env\n",
    "    algo = args.algo\n",
    "    folder = args.folder\n",
    "\n",
    "    try:\n",
    "        _, model_path, log_path = get_model_path(\n",
    "            args.exp_id,\n",
    "            folder,\n",
    "            algo,\n",
    "            env_name,\n",
    "            args.load_best,\n",
    "            args.load_checkpoint,\n",
    "            args.load_last_checkpoint,\n",
    "        )\n",
    "    except (AssertionError, ValueError) as e:\n",
    "        # Special case for rl-trained agents\n",
    "        # auto-download from the hub\n",
    "        if \"rl-trained-agents\" not in folder:\n",
    "            raise e\n",
    "        else:\n",
    "            print(\"Pretrained model not found, trying to download it from sb3 Huggingface hub: https://huggingface.co/sb3\")\n",
    "            # Auto-download\n",
    "            download_from_hub(\n",
    "                algo=algo,\n",
    "                env_name=env_name,\n",
    "                exp_id=args.exp_id,\n",
    "                folder=folder,\n",
    "                organization=\"sb3\",\n",
    "                repo_name=None,\n",
    "                force=False,\n",
    "            )\n",
    "            # Try again\n",
    "            _, model_path, log_path = get_model_path(\n",
    "                args.exp_id,\n",
    "                folder,\n",
    "                algo,\n",
    "                env_name,\n",
    "                args.load_best,\n",
    "                args.load_checkpoint,\n",
    "                args.load_last_checkpoint,\n",
    "            )\n",
    "\n",
    "    print(f\"Loading {model_path}\")\n",
    "\n",
    "    # Off-policy algorithm only support one env for now\n",
    "    off_policy_algos = [\"qrdqn\", \"dqn\", \"ddpg\", \"sac\", \"her\", \"td3\", \"tqc\"]\n",
    "\n",
    "    if algo in off_policy_algos:\n",
    "        args.n_envs = 1\n",
    "\n",
    "    set_random_seed(args.seed)\n",
    "\n",
    "    if args.num_threads > 0:\n",
    "        if args.verbose > 1:\n",
    "            print(f\"Setting torch.num_threads to {args.num_threads}\")\n",
    "        th.set_num_threads(args.num_threads)\n",
    "\n",
    "    is_atari = ExperimentManager.is_atari(env_name.gym_id)\n",
    "    is_minigrid = ExperimentManager.is_minigrid(env_name.gym_id)\n",
    "\n",
    "    stats_path = os.path.join(log_path, env_name)\n",
    "    hyperparams, maybe_stats_path = get_saved_hyperparams(stats_path, norm_reward=args.norm_reward, test_mode=True)\n",
    "\n",
    "    # load env_kwargs if existing\n",
    "    env_kwargs = {}\n",
    "    args_path = os.path.join(log_path, env_name, \"args.yml\")\n",
    "    if os.path.isfile(args_path):\n",
    "        with open(args_path) as f:\n",
    "            loaded_args = yaml.load(f, Loader=yaml.UnsafeLoader)  # pytype: disable=module-attr\n",
    "            if loaded_args[\"env_kwargs\"] is not None:\n",
    "                env_kwargs = loaded_args[\"env_kwargs\"]\n",
    "    # overwrite with command line arguments\n",
    "    if args.env_kwargs is not None:\n",
    "        env_kwargs.update(args.env_kwargs)\n",
    "\n",
    "    log_dir = args.reward_log if args.reward_log != \"\" else None\n",
    "\n",
    "    env = create_test_env(\n",
    "        env_name.gym_id,\n",
    "        n_envs=args.n_envs,\n",
    "        stats_path=maybe_stats_path,\n",
    "        seed=args.seed,\n",
    "        log_dir=log_dir,\n",
    "        should_render=not args.no_render if not args.render_rgb else False, # jskang \n",
    "        hyperparams=hyperparams,\n",
    "        env_kwargs=env_kwargs,\n",
    "    )\n",
    "\n",
    "    kwargs = dict(seed=args.seed)\n",
    "    if algo in off_policy_algos:\n",
    "        # Dummy buffer size as we don't need memory to enjoy the trained agent\n",
    "        kwargs.update(dict(buffer_size=1))\n",
    "        # Hack due to breaking change in v1.6\n",
    "        # handle_timeout_termination cannot be at the same time\n",
    "        # with optimize_memory_usage\n",
    "        if \"optimize_memory_usage\" in hyperparams:\n",
    "            kwargs.update(optimize_memory_usage=False)\n",
    "\n",
    "    # Check if we are running python 3.8+\n",
    "    # we need to patch saved model under python 3.6/3.7 to load them\n",
    "    newer_python_version = sys.version_info.major == 3 and sys.version_info.minor >= 8\n",
    "\n",
    "    custom_objects = {}\n",
    "    if newer_python_version or args.custom_objects:\n",
    "        custom_objects = {\n",
    "            \"learning_rate\": 0.0,\n",
    "            \"lr_schedule\": lambda _: 0.0,\n",
    "            \"clip_range\": lambda _: 0.0,\n",
    "        }\n",
    "\n",
    "    if \"HerReplayBuffer\" in hyperparams.get(\"replay_buffer_class\", \"\"):\n",
    "        kwargs[\"env\"] = env\n",
    "\n",
    "    model = ALGOS[algo].load(model_path, custom_objects=custom_objects, device=args.device, **kwargs)\n",
    "    #print(env.render_mode)\n",
    "    #env.render_mode = 'rgb_array'\n",
    "    #print(env.render_mode)\n",
    "\n",
    "    obs = env.reset()\n",
    "\n",
    "    # Deterministic by default except for atari games\n",
    "    stochastic = args.stochastic or (is_atari or is_minigrid) and not args.deterministic\n",
    "    deterministic = not stochastic\n",
    "\n",
    "    episode_reward = 0.0\n",
    "    episode_rewards, episode_lengths = [], []\n",
    "    ep_len = 0\n",
    "    # For HER, monitor success rate\n",
    "    successes = []\n",
    "    lstm_states = None\n",
    "    episode_start = np.ones((env.num_envs,), dtype=bool)\n",
    "\n",
    "    generator = range(args.n_timesteps)\n",
    "    if args.progress:\n",
    "        if tqdm is None:\n",
    "            raise ImportError(\"Please install tqdm and rich to use the progress bar\")\n",
    "        generator = tqdm(generator)\n",
    "\n",
    "    try:\n",
    "        frames = []\n",
    "        import mediapy as media\n",
    "        for _ in generator:\n",
    "            action, lstm_states = model.predict(\n",
    "                obs,  # type: ignore[arg-type]\n",
    "                state=lstm_states,\n",
    "                episode_start=episode_start,\n",
    "                deterministic=deterministic,\n",
    "            )\n",
    "            obs, reward, done, infos = env.step(action)\n",
    "\n",
    "            episode_start = done\n",
    "\n",
    "            if not args.no_render:\n",
    "                #env.render(\"human\")\n",
    "                pass\n",
    "            if args.render_rgb :\n",
    "                frame = env.render('rgb_array')\n",
    "                #frame = env.render()\n",
    "                pass\n",
    "                print(frame)\n",
    "                frames.append(frame)\n",
    "\n",
    "            episode_reward += reward[0]\n",
    "            ep_len += 1\n",
    "\n",
    "            if args.n_envs == 1:\n",
    "                # For atari the return reward is not the atari score\n",
    "                # so we have to get it from the infos dict\n",
    "                if is_atari and infos is not None and args.verbose >= 1:\n",
    "                    episode_infos = infos[0].get(\"episode\")\n",
    "                    if episode_infos is not None:\n",
    "                        print(f\"Atari Episode Score: {episode_infos['r']:.2f}\")\n",
    "                        print(\"Atari Episode Length\", episode_infos[\"l\"])\n",
    "\n",
    "                if done and not is_atari and args.verbose > 0:\n",
    "                    # NOTE: for env using VecNormalize, the mean reward\n",
    "                    # is a normalized reward when `--norm_reward` flag is passed\n",
    "                    print(f\"Episode Reward: {episode_reward:.2f}\")\n",
    "                    print(\"Episode Length\", ep_len)\n",
    "                    episode_rewards.append(episode_reward)\n",
    "                    episode_lengths.append(ep_len)\n",
    "                    episode_reward = 0.0\n",
    "                    ep_len = 0\n",
    "\n",
    "                # Reset also when the goal is achieved when using HER\n",
    "                if done and infos[0].get(\"is_success\") is not None:\n",
    "                    if args.verbose > 1:\n",
    "                        print(\"Success?\", infos[0].get(\"is_success\", False))\n",
    "\n",
    "                    if infos[0].get(\"is_success\") is not None:\n",
    "                        successes.append(infos[0].get(\"is_success\", False))\n",
    "                        episode_reward, ep_len = 0.0, 0\n",
    "        media.show_video(frames, fps=30)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    if args.verbose > 0 and len(successes) > 0:\n",
    "        print(f\"Success rate: {100 * np.mean(successes):.2f}%\")\n",
    "\n",
    "    if args.verbose > 0 and len(episode_rewards) > 0:\n",
    "        print(f\"{len(episode_rewards)} Episodes\")\n",
    "        print(f\"Mean reward: {np.mean(episode_rewards):.2f} +/- {np.std(episode_rewards):.2f}\")\n",
    "\n",
    "    if args.verbose > 0 and len(episode_lengths) > 0:\n",
    "        print(f\"Mean episode length: {np.mean(episode_lengths):.2f} +/- {np.std(episode_lengths):.2f}\")\n",
    "\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov 11 05:15:32 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.147.05   Driver Version: 525.147.05   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\n",
      "|  0%   49C    P2    64W / 175W |    898MiB /  8192MiB |     27%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n",
      "Setting environment variable to use GPU rendering:\n",
      "env: MUJOCO_GL=egl\n",
      "Checking that the installation succeeded:\n",
      "Installation successful.\n",
      "Installing mediapy:\n"
     ]
    }
   ],
   "source": [
    "#@title Check if installation was successful\n",
    "\n",
    "#from google.colab import files\n",
    "\n",
    "import distutils.util\n",
    "import os\n",
    "import subprocess\n",
    "if subprocess.run('nvidia-smi').returncode:\n",
    "  raise RuntimeError(\n",
    "      'Cannot communicate with GPU. '\n",
    "      'Make sure you are using a GPU Colab runtime. '\n",
    "      'Go to the Runtime menu and select Choose runtime type.')\n",
    "\n",
    "# Add an ICD config so that glvnd can pick up the Nvidia EGL driver.\n",
    "# This is usually installed as part of an Nvidia driver package, but the Colab\n",
    "# kernel doesn't install its driver via APT, and as a result the ICD is missing.\n",
    "# (https://github.com/NVIDIA/libglvnd/blob/master/src/EGL/icd_enumeration.md)\n",
    "NVIDIA_ICD_CONFIG_PATH = '/usr/share/glvnd/egl_vendor.d/10_nvidia.json'\n",
    "if not os.path.exists(NVIDIA_ICD_CONFIG_PATH):\n",
    "  with open(NVIDIA_ICD_CONFIG_PATH, 'w') as f:\n",
    "    f.write(\"\"\"{\n",
    "    \"file_format_version\" : \"1.0.0\",\n",
    "    \"ICD\" : {\n",
    "        \"library_path\" : \"libEGL_nvidia.so.0\"\n",
    "    }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "# Configure MuJoCo to use the EGL rendering backend (requires GPU)\n",
    "print('Setting environment variable to use GPU rendering:')\n",
    "%env MUJOCO_GL=egl\n",
    "\n",
    "try:\n",
    "  print('Checking that the installation succeeded:')\n",
    "  import mujoco\n",
    "  mujoco.MjModel.from_xml_string('<mujoco/>')\n",
    "except Exception as e:\n",
    "  raise e from RuntimeError(\n",
    "      'Something went wrong during installation. Check the shell output above '\n",
    "      'for more information.\\n'\n",
    "      'If using a hosted Colab runtime, make sure you enable GPU acceleration '\n",
    "      'by going to the Runtime menu and selecting \"Choose runtime type\".')\n",
    "\n",
    "print('Installation successful.')\n",
    "\n",
    "#@title Import packages for plotting and creating graphics\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "from typing import Callable, NamedTuple, Optional, Union, List\n",
    "\n",
    "# Graphics and plotting.\n",
    "print('Installing mediapy:')\n",
    "!command -v ffmpeg >/dev/null || (apt update && apt install -y ffmpeg)\n",
    "!pip install -q mediapy\n",
    "import mediapy as media\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# More legible printing from numpy.\n",
    "np.set_printoptions(precision=3, suppress=True, linewidth=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DISPLAY=897985abde1b:10.0\n"
     ]
    }
   ],
   "source": [
    "%env DISPLAY=897985abde1b:10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'\\nimport random\\nimport gymnasium as gym\\nenv = gym.make('Hopper', render_mode='rgb_array')\\nenv.reset()\\nn_frames = 120\\nheight = 480\\nwidth = 480\\nframes = []\\n#import mujoco\\nmodel = env.model\\ndata = env.data\\n\\nmujoco.mj_step(model, data)\\nrenderer = mujoco.Renderer(model, height, width)\\nmujoco.mj_resetDataKeyframe(model, data,0)\\n\\njoint_names = ['thigh_joint', 'leg_joint', 'foot_joint']\\njoint_ranges = [model.joint(joint).range for joint in joint_names]\\nfor i in range(n_frames):\\n  while data.time < i/30.0:   \\n    new_action = []\\n        \\n    # ob, reward, terminated, False, {}\\n    obs, reward, terminated, _, _ = env.step(new_action)\\n    #mujoco.mj_step(model, data)\\n  renderer.update_scene(data)\\n  frame = renderer.render()\\n  frames.append(frame)\\nmedia.show_video(frames, fps=30)\\n\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''''\n",
    "import random\n",
    "import gymnasium as gym\n",
    "env = gym.make('Hopper', render_mode='rgb_array')\n",
    "env.reset()\n",
    "n_frames = 120\n",
    "height = 480\n",
    "width = 480\n",
    "frames = []\n",
    "#import mujoco\n",
    "model = env.model\n",
    "data = env.data\n",
    "\n",
    "mujoco.mj_step(model, data)\n",
    "renderer = mujoco.Renderer(model, height, width)\n",
    "mujoco.mj_resetDataKeyframe(model, data,0)\n",
    "\n",
    "joint_names = ['thigh_joint', 'leg_joint', 'foot_joint']\n",
    "joint_ranges = [model.joint(joint).range for joint in joint_names]\n",
    "for i in range(n_frames):\n",
    "  while data.time < i/30.0:   \n",
    "    new_action = []\n",
    "        \n",
    "    # ob, reward, terminated, False, {}\n",
    "    obs, reward, terminated, _, _ = env.step(new_action)\n",
    "    #mujoco.mj_step(model, data)\n",
    "  renderer.update_scene(data)\n",
    "  frame = renderer.render()\n",
    "  frames.append(frame)\n",
    "media.show_video(frames, fps=30)\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading latest experiment, id=1\n",
      "Loading ./logs/tqc/FetchPickAndPlace-v1_1/rl_model_110000_steps.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/env2/lib/python3.10/site-packages/gymnasium_robotics/envs/robot_env.py:361: UserWarning: \u001b[33mWARN: This version of the mujoco environments depends on the mujoco-py bindings, which are no longer maintained and may stop working. Please upgrade to the v4 versions of the environments (which depend on the mujoco python bindings instead), unless you are trying to precisely replicate previous works).\u001b[0m\n",
      "  logger.warn(\n",
      "/env2/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:233: UserWarning: Starting from gymnasium v0.26, render modes are determined during the initialization of the environment.\n",
      "                We allow to pass a mode argument to maintain a backwards compatible VecEnv API, but the mode (rgb_array)\n",
      "                has to be the same as the environment render mode (None) which is not the case.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Episode Reward: -49.00\n",
      "Episode Length 50\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Episode Reward: -50.00\n",
      "Episode Length 50\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Episode Reward: -50.00\n",
      "Episode Length 50\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Episode Reward: -50.00\n",
      "Episode Length 50\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Episode Reward: -50.00\n",
      "Episode Length 50\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Episode Reward: -50.00\n",
      "Episode Length 50\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Episode Reward: -50.00\n",
      "Episode Length 50\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Episode Reward: -47.00\n",
      "Episode Length 50\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Episode Reward: -50.00\n",
      "Episode Length 50\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Episode Reward: -50.00\n",
      "Episode Length 50\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Episode Reward: -50.00\n",
      "Episode Length 50\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Episode Reward: -50.00\n",
      "Episode Length 50\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Episode Reward: -50.00\n",
      "Episode Length 50\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Episode Reward: -50.00\n",
      "Episode Length 50\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Episode Reward: -50.00\n",
      "Episode Length 50\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Episode Reward: -15.00\n",
      "Episode Length 50\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Episode Reward: -50.00\n",
      "Episode Length 50\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Episode Reward: -48.00\n",
      "Episode Length 50\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Episode Reward: -4.00\n",
      "Episode Length 50\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Episode Reward: -46.00\n",
      "Episode Length 50\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/research/rl_zoo_jesnk/enjoy.ipynb Cell 5\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfisher-docker/research/rl_zoo_jesnk/enjoy.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=65'>66</a>\u001b[0m args\u001b[39m.\u001b[39mrender_rgb \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfisher-docker/research/rl_zoo_jesnk/enjoy.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m args\u001b[39m.\u001b[39mfolder \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m./logs/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bfisher-docker/research/rl_zoo_jesnk/enjoy.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m enjoy(args)\n",
      "\u001b[1;32m/research/rl_zoo_jesnk/enjoy.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bfisher-docker/research/rl_zoo_jesnk/enjoy.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=214'>215</a>\u001b[0m                     successes\u001b[39m.\u001b[39mappend(infos[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mis_success\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m))\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bfisher-docker/research/rl_zoo_jesnk/enjoy.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=215'>216</a>\u001b[0m                     episode_reward, ep_len \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m, \u001b[39m0\u001b[39m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bfisher-docker/research/rl_zoo_jesnk/enjoy.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=216'>217</a>\u001b[0m     media\u001b[39m.\u001b[39;49mshow_video(frames, fps\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bfisher-docker/research/rl_zoo_jesnk/enjoy.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=218'>219</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bfisher-docker/research/rl_zoo_jesnk/enjoy.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=219'>220</a>\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m/env2/lib/python3.10/site-packages/mediapy/__init__.py:1848\u001b[0m, in \u001b[0;36mshow_video\u001b[0;34m(images, title, **kwargs)\u001b[0m\n\u001b[1;32m   1825\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshow_video\u001b[39m(\n\u001b[1;32m   1826\u001b[0m     images: Iterable[_NDArray], \u001b[39m*\u001b[39m, title: \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any\n\u001b[1;32m   1827\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1828\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Displays a video in the IPython notebook and optionally saves it to a file.\u001b[39;00m\n\u001b[1;32m   1829\u001b[0m \n\u001b[1;32m   1830\u001b[0m \u001b[39m  See `show_videos`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[39m    html string if `return_html` is `True`.\u001b[39;00m\n\u001b[1;32m   1847\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1848\u001b[0m   \u001b[39mreturn\u001b[39;00m show_videos([images], [title], \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/env2/lib/python3.10/site-packages/mediapy/__init__.py:1926\u001b[0m, in \u001b[0;36mshow_videos\u001b[0;34m(videos, titles, width, height, downsample, columns, fps, bps, qp, codec, ylabel, html_class, return_html, **kwargs)\u001b[0m\n\u001b[1;32m   1923\u001b[0m metadata: VideoMetadata \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(video, \u001b[39m'\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m   1924\u001b[0m first_image, video \u001b[39m=\u001b[39m _peek_first(video)\n\u001b[1;32m   1925\u001b[0m w, h \u001b[39m=\u001b[39m _get_width_height(\n\u001b[0;32m-> 1926\u001b[0m     width, height, first_image\u001b[39m.\u001b[39;49mshape[:\u001b[39m2\u001b[39m]  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   1927\u001b[0m )\n\u001b[1;32m   1928\u001b[0m \u001b[39mif\u001b[39;00m downsample \u001b[39mand\u001b[39;00m (w \u001b[39m<\u001b[39m first_image\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39mor\u001b[39;00m h \u001b[39m<\u001b[39m first_image\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):  \u001b[39m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[1;32m   1929\u001b[0m   \u001b[39m# Not resize_video() because each image may have different depth and type.\u001b[39;00m\n\u001b[1;32m   1930\u001b[0m   video \u001b[39m=\u001b[39m [resize_image(image, (h, w)) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m video]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--env\", help=\"environment ID\", type=EnvironmentName, default=\"CartPole-v1\")\n",
    "parser.add_argument(\"-f\", \"--folder\", help=\"Log folder\", type=str, default=\"rl-trained-agents\")\n",
    "parser.add_argument(\"--algo\", help=\"RL Algorithm\", default=\"ppo\", type=str, required=False, choices=list(ALGOS.keys()))\n",
    "parser.add_argument(\"-n\", \"--n-timesteps\", help=\"number of timesteps\", default=1000, type=int)\n",
    "parser.add_argument(\"--num-threads\", help=\"Number of threads for PyTorch (-1 to use default)\", default=-1, type=int)\n",
    "parser.add_argument(\"--n-envs\", help=\"number of environments\", default=1, type=int)\n",
    "parser.add_argument(\"--exp-id\", help=\"Experiment ID (default: 0: latest, -1: no exp folder)\", default=0, type=int)\n",
    "parser.add_argument(\"--verbose\", help=\"Verbose mode (0: no output, 1: INFO)\", default=1, type=int)\n",
    "parser.add_argument(\n",
    "    \"--no-render\", action=\"store_true\", default=False, help=\"Do not render the environment (useful for tests)\"\n",
    ")\n",
    "parser.add_argument(\"--deterministic\", action=\"store_true\", default=False, help=\"Use deterministic actions\")\n",
    "parser.add_argument(\"--device\", help=\"PyTorch device to be use (ex: cpu, cuda...)\", default=\"auto\", type=str)\n",
    "parser.add_argument(\n",
    "    \"--load-best\", action=\"store_true\", default=False, help=\"Load best model instead of last model if available\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--load-checkpoint\",\n",
    "    type=int,\n",
    "    help=\"Load checkpoint instead of last model if available, \"\n",
    "    \"you must pass the number of timesteps corresponding to it\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--load-last-checkpoint\",\n",
    "    action=\"store_true\",\n",
    "    default=False,\n",
    "    help=\"Load last checkpoint instead of last model if available\",\n",
    ")\n",
    "parser.add_argument(\"--stochastic\", action=\"store_true\", default=False, help=\"Use stochastic actions\")\n",
    "parser.add_argument(\n",
    "    \"--norm-reward\", action=\"store_true\", default=False, help=\"Normalize reward if applicable (trained with VecNormalize)\"\n",
    ")\n",
    "parser.add_argument(\"--seed\", help=\"Random generator seed\", type=int, default=0)\n",
    "parser.add_argument(\"--reward-log\", help=\"Where to log reward\", default=\"\", type=str)\n",
    "parser.add_argument(\n",
    "    \"--gym-packages\",\n",
    "    type=str,\n",
    "    nargs=\"+\",\n",
    "    default=[],\n",
    "    help=\"Additional external Gym environment package modules to import\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--env-kwargs\", type=str, nargs=\"+\", action=StoreDict, help=\"Optional keyword argument to pass to the env constructor\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--custom-objects\", action=\"store_true\", default=False, help=\"Use custom objects to solve loading issues\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"-P\",\n",
    "    \"--progress\",\n",
    "    action=\"store_true\",\n",
    "    default=False,\n",
    "    help=\"if toggled, display a progress bar using tqdm and rich\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--render_rgb\", action='store_true', default=False, help=\"if toggled, render rgb array\"\n",
    ")\n",
    "#args = parser.parse_args()\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "args.env = EnvironmentName('FetchPickAndPlace-v1')\n",
    "args.algo = 'tqc'\n",
    "args.load_last_checkpoint = True\n",
    "args.no_render = True\n",
    "args.render_rgb = True\n",
    "args.folder = './logs/'\n",
    "\n",
    "enjoy(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Wrapper.render() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/research/rl_zoo_jesnk/enjoy.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bfisher-docker/research/rl_zoo_jesnk/enjoy.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m env\u001b[39m.\u001b[39;49mrender(\u001b[39m\"\u001b[39;49m\u001b[39mrgb_array\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Wrapper.render() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "env.render(\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

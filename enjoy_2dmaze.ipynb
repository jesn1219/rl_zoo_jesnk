{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/env2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import argparse\n",
    "import importlib\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import yaml\n",
    "from huggingface_sb3 import EnvironmentName\n",
    "from stable_baselines3.common.callbacks import tqdm\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "\n",
    "import rl_zoo3.import_envs  # noqa: F401 pylint: disable=unused-import\n",
    "from rl_zoo3 import ALGOS, create_test_env, get_saved_hyperparams\n",
    "from rl_zoo3.exp_manager import ExperimentManager\n",
    "from rl_zoo3.load_from_hub import download_from_hub\n",
    "from rl_zoo3.utils import StoreDict, get_model_path\n",
    "import cv2\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def enjoy(args) -> None:  # noqa: C901\n",
    "    # Going through custom gym packages to let them register in the global registory\n",
    "    for env_module in args.gym_packages:\n",
    "        importlib.import_module(env_module)\n",
    "\n",
    "    env_name: EnvironmentName = args.env\n",
    "    algo = args.algo\n",
    "    folder = args.folder\n",
    "\n",
    "    try:\n",
    "        _, model_path, log_path = get_model_path(\n",
    "            args.exp_id,\n",
    "            folder,\n",
    "            algo,\n",
    "            env_name,\n",
    "            args.load_best,\n",
    "            args.load_checkpoint,\n",
    "            args.load_last_checkpoint,\n",
    "        )\n",
    "    except (AssertionError, ValueError) as e:\n",
    "        # Special case for rl-trained agents\n",
    "        # auto-download from the hub\n",
    "        if \"rl-trained-agents\" not in folder:\n",
    "            raise e\n",
    "        else:\n",
    "            print(\"Pretrained model not found, trying to download it from sb3 Huggingface hub: https://huggingface.co/sb3\")\n",
    "            # Auto-download\n",
    "            download_from_hub(\n",
    "                algo=algo,\n",
    "                env_name=env_name,\n",
    "                exp_id=args.exp_id,\n",
    "                folder=folder,\n",
    "                organization=\"sb3\",\n",
    "                repo_name=None,\n",
    "                force=False,\n",
    "            )\n",
    "            # Try again\n",
    "            _, model_path, log_path = get_model_path(\n",
    "                args.exp_id,\n",
    "                folder,\n",
    "                algo,\n",
    "                env_name,\n",
    "                args.load_best,\n",
    "                args.load_checkpoint,\n",
    "                args.load_last_checkpoint,\n",
    "            )\n",
    "\n",
    "    print(f\"Loading {model_path}\")\n",
    "\n",
    "    # Off-policy algorithm only support one env for now\n",
    "    off_policy_algos = [\"qrdqn\", \"dqn\", \"ddpg\", \"sac\", \"her\", \"td3\", \"tqc\"]\n",
    "\n",
    "    if algo in off_policy_algos:\n",
    "        args.n_envs = 1\n",
    "\n",
    "    set_random_seed(args.seed)\n",
    "\n",
    "    if args.num_threads > 0:\n",
    "        if args.verbose > 1:\n",
    "            print(f\"Setting torch.num_threads to {args.num_threads}\")\n",
    "        th.set_num_threads(args.num_threads)\n",
    "\n",
    "    is_atari = ExperimentManager.is_atari(env_name.gym_id)\n",
    "    is_minigrid = ExperimentManager.is_minigrid(env_name.gym_id)\n",
    "\n",
    "    stats_path = os.path.join(log_path, env_name)\n",
    "    hyperparams, maybe_stats_path = get_saved_hyperparams(stats_path, norm_reward=args.norm_reward, test_mode=True)\n",
    "\n",
    "    # load env_kwargs if existing\n",
    "    env_kwargs = {}\n",
    "    args_path = os.path.join(log_path, env_name, \"args.yml\")\n",
    "    if os.path.isfile(args_path):\n",
    "        with open(args_path) as f:\n",
    "            loaded_args = yaml.load(f, Loader=yaml.UnsafeLoader)  # pytype: disable=module-attr\n",
    "            if loaded_args[\"env_kwargs\"] is not None:\n",
    "                env_kwargs = loaded_args[\"env_kwargs\"]\n",
    "    # overwrite with command line arguments\n",
    "    if args.env_kwargs is not None:\n",
    "        env_kwargs.update(args.env_kwargs)\n",
    "\n",
    "    log_dir = args.reward_log if args.reward_log != \"\" else None\n",
    "\n",
    "    env = create_test_env(\n",
    "        env_name.gym_id,\n",
    "        n_envs=args.n_envs,\n",
    "        stats_path=maybe_stats_path,\n",
    "        seed=args.seed,\n",
    "        log_dir=log_dir,\n",
    "        should_render=not args.no_render if not args.render_rgb else False, # jskang \n",
    "        hyperparams=hyperparams,\n",
    "        env_kwargs=env_kwargs,\n",
    "    )\n",
    "\n",
    "    kwargs = dict(seed=args.seed)\n",
    "    if algo in off_policy_algos:\n",
    "        # Dummy buffer size as we don't need memory to enjoy the trained agent\n",
    "        kwargs.update(dict(buffer_size=1))\n",
    "        # Hack due to breaking change in v1.6\n",
    "        # handle_timeout_termination cannot be at the same time\n",
    "        # with optimize_memory_usage\n",
    "        if \"optimize_memory_usage\" in hyperparams:\n",
    "            kwargs.update(optimize_memory_usage=False)\n",
    "\n",
    "    # Check if we are running python 3.8+\n",
    "    # we need to patch saved model under python 3.6/3.7 to load them\n",
    "    newer_python_version = sys.version_info.major == 3 and sys.version_info.minor >= 8\n",
    "\n",
    "    custom_objects = {}\n",
    "    if newer_python_version or args.custom_objects:\n",
    "        custom_objects = {\n",
    "            \"learning_rate\": 0.0,\n",
    "            \"lr_schedule\": lambda _: 0.0,\n",
    "            \"clip_range\": lambda _: 0.0,\n",
    "        }\n",
    "\n",
    "    if \"HerReplayBuffer\" in hyperparams.get(\"replay_buffer_class\", \"\"):\n",
    "        kwargs[\"env\"] = env\n",
    "\n",
    "    model = ALGOS[algo].load(model_path, custom_objects=custom_objects, device=args.device, **kwargs)\n",
    "    #print(env.render_mode)\n",
    "    #env.render_mode = 'rgb_array'\n",
    "    #print(env.render_mode)\n",
    "\n",
    "    obs = env.reset()\n",
    "\n",
    "    # Deterministic by default except for atari games\n",
    "    stochastic = args.stochastic or (is_atari or is_minigrid) and not args.deterministic\n",
    "    deterministic = not stochastic\n",
    "\n",
    "    episode_reward = 0.0\n",
    "    episode_rewards, episode_lengths = [], []\n",
    "    ep_len = 0\n",
    "    # For HER, monitor success rate\n",
    "    successes = []\n",
    "    lstm_states = None\n",
    "    episode_start = np.ones((env.num_envs,), dtype=bool)\n",
    "\n",
    "    generator = range(args.n_timesteps)\n",
    "    if args.progress:\n",
    "        if tqdm is None:\n",
    "            raise ImportError(\"Please install tqdm and rich to use the progress bar\")\n",
    "        generator = tqdm(generator)\n",
    "\n",
    "    try:\n",
    "        frames = []\n",
    "        initial_agent_pos = None\n",
    "        import mediapy as media\n",
    "        for _ in generator:\n",
    "            action, lstm_states = model.predict(\n",
    "                obs,  # type: ignore[arg-type]\n",
    "                state=lstm_states,\n",
    "                episode_start=episode_start,\n",
    "                deterministic=deterministic,\n",
    "            )\n",
    "\n",
    "            obs, reward, done, infos = env.step(action)\n",
    "            if initial_agent_pos is None:\n",
    "                initial_agent_pos = obs['achieved_goal']\n",
    "                desired_goal = obs['desired_goal']\n",
    "            #print(obs)\n",
    "            #print(obs['achieved_goal'])\n",
    "            #print(obs['desired_goal'])\n",
    "            #print(action)\n",
    "\n",
    "            episode_start = done\n",
    "\n",
    "            if not args.no_render:\n",
    "                #env.render(\"human\")\n",
    "                pass\n",
    "            if args.render_rgb :\n",
    "                frame = env.render('rgb_array')\n",
    "                #frame = env.render()\n",
    "                # add episode number\n",
    "                frame = cv2.putText(frame, f\"episode: {_}\", (10, 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "                frame = cv2.putText(frame, f\"reward: {reward[0]:.2f}, goal: {obs['desired_goal']}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "                frame = cv2.putText(frame, f\"done: {done}, agent: {obs['achieved_goal']}\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "                frame = cv2.putText(frame, f\"episode_reward: {episode_reward:.2f}\", (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "                frame = cv2.putText(frame, f\"episode_length: {ep_len:.2f}\", (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "                if infos is not None:\n",
    "                    frame = cv2.putText(frame, f\"infos: {infos[0]}\", (10, 110), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "                frames.append(frame)\n",
    "                #print(type(model))\n",
    "            \n",
    "            \n",
    "            def plot_qvalue(initial_agent_pos, goal):\n",
    "                dummy_state = [0.,0.]\n",
    "                dummy_x_range = [-5, 5]\n",
    "                dummy_y_range = [-5, 5]\n",
    "                num_grid = 50\n",
    "                dummy_action = [0.,0.]\n",
    "\n",
    "\n",
    "                # split the x,y range into 30x30 grid\n",
    "                dummy_x_grid = np.linspace(dummy_x_range[0], dummy_x_range[1], num_grid)\n",
    "                dummy_y_grid = np.linspace(dummy_y_range[0], dummy_y_range[1], num_grid)\n",
    "\n",
    "                # Initialize a 2D array to store the qvalues\n",
    "                qvalues = np.zeros((num_grid, num_grid))\n",
    "\n",
    "                for i, x in enumerate(dummy_x_grid):\n",
    "                    for j, y in enumerate(dummy_y_grid):\n",
    "                        dummy_state = [x,y]\n",
    "                        dummy_observation = [ dummy_state + [0.,0.] ]\n",
    "                        dummy_obs = OrderedDict([ ('achieved_goal', dummy_state), ('desired_goal', goal),('observation', dummy_observation)])\n",
    "\n",
    "                        th_obs, _ = model.policy.obs_to_tensor(dummy_obs)\n",
    "                        \n",
    "                        th_action = th.tensor(dummy_action, dtype=th.float32).unsqueeze(0).to(model.device)\n",
    "                        \n",
    "                        current_qvalue = model.critic(th_obs,th_action)\n",
    "                        if isinstance(current_qvalue,tuple) :\n",
    "                            current_qvalue = np.array(th.tensor(current_qvalue).cpu()).mean()\n",
    "                        else :\n",
    "                            current_qvalue = current_qvalue.mean().item()\n",
    "\n",
    "                        # Store the qvalue\n",
    "                        qvalues[-j, i] = current_qvalue\n",
    "                # get closest grid to the goal\n",
    "                goal = goal\n",
    "                goal_x = goal[0][0]\n",
    "                goal_y = goal[0][1]\n",
    "                initial_agent_x = initial_agent_pos[0][0]\n",
    "                initial_agent_y = initial_agent_pos[0][1]\n",
    "                \n",
    "                goal_x_idx = np.argmin(np.abs(dummy_x_grid - goal_x))\n",
    "                goal_y_idx = np.argmin(np.abs(dummy_y_grid - goal_y))\n",
    "                initial_agent_x_idx = np.argmin(np.abs(dummy_x_grid - initial_agent_x))\n",
    "                initial_agent_y_idx = np.argmin(np.abs(dummy_y_grid - initial_agent_y))\n",
    "\n",
    "                # 이미지의 y축 좌표를 반전시켜서 scatter에 적용\n",
    "                adjusted_goal_y_idx = qvalues.shape[0] - 1 - goal_y_idx\n",
    "                adjusted_initial_agent_y_idx = qvalues.shape[0] - 1 - initial_agent_y_idx\n",
    "\n",
    "                # Plot the qvalues\n",
    "                plt.imshow(qvalues, cmap='hot', interpolation='nearest')\n",
    "                # Add 'X' marker at the goal\n",
    "                plt.scatter(goal_x_idx, adjusted_goal_y_idx, c='blue', marker='x')\n",
    "                plt.scatter(initial_agent_x_idx, adjusted_initial_agent_y_idx, c='green', marker='o')\n",
    "                plt.colorbar()\n",
    "\n",
    "                plt.show()\n",
    "\n",
    "                \n",
    "            \n",
    "            episode_reward += reward[0]\n",
    "            ep_len += 1\n",
    "\n",
    "            if args.n_envs == 1:\n",
    "                # For atari the return reward is not the atari score\n",
    "                # so we have to get it from the infos dict\n",
    "                if is_atari and infos is not None and args.verbose >= 1:\n",
    "                    episode_infos = infos[0].get(\"episode\")\n",
    "                    if episode_infos is not None:\n",
    "                        print(f\"Atari Episode Score: {episode_infos['r']:.2f}\")\n",
    "                        print(\"Atari Episode Length\", episode_infos[\"l\"])\n",
    "\n",
    "                if done and not is_atari and args.verbose > 0:\n",
    "                    # NOTE: for env using VecNormalize, the mean reward\n",
    "                    # is a normalized reward when `--norm_reward` flag is passed\n",
    "                    print(f\"Episode Reward: {episode_reward:.2f}\")\n",
    "                    print(\"Episode Length\", ep_len)\n",
    "                    print(f'initial_agent {initial_agent_pos}, goal:{desired_goal}')\n",
    "                    episode_rewards.append(episode_reward)\n",
    "                    episode_lengths.append(ep_len)\n",
    "                    episode_reward = 0.0\n",
    "                    ep_len = 0\n",
    "\n",
    "\n",
    "                # Reset also when the goal is achieved when using HER\n",
    "                if done and infos[0].get(\"is_success\") is not None:\n",
    "                    if args.verbose > 1:\n",
    "                        print(\"Success?\", infos[0].get(\"is_success\", False))\n",
    "\n",
    "                    if infos[0].get(\"is_success\") is not None:\n",
    "                        successes.append(infos[0].get(\"is_success\", False))\n",
    "                        episode_reward, ep_len = 0.0, 0\n",
    "                \n",
    "                if done :\n",
    "                    plot_qvalue(initial_agent_pos, desired_goal)\n",
    "                    initial_agent_pos = None\n",
    "                \n",
    "        media.show_video(frames, fps=30)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    if args.verbose > 0 and len(successes) > 0:\n",
    "        print(f\"Success rate: {100 * np.mean(successes):.2f}%\")\n",
    "\n",
    "    if args.verbose > 0 and len(episode_rewards) > 0:\n",
    "        print(f\"{len(episode_rewards)} Episodes\")\n",
    "        print(f\"Mean reward: {np.mean(episode_rewards):.2f} +/- {np.std(episode_rewards):.2f}\")\n",
    "\n",
    "    if args.verbose > 0 and len(episode_lengths) > 0:\n",
    "        print(f\"Mean episode length: {np.mean(episode_lengths):.2f} +/- {np.std(episode_lengths):.2f}\")\n",
    "\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 22 14:57:32 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.147.05   Driver Version: 525.147.05   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |\n",
      "| 43%   56C    P2   121W / 175W |   6513MiB /  8192MiB |     95%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n",
      "Setting environment variable to use GPU rendering:\n",
      "env: MUJOCO_GL=egl\n",
      "Checking that the installation succeeded:\n",
      "Installation successful.\n",
      "Installing mediapy:\n"
     ]
    }
   ],
   "source": [
    "#@title Check if installation was successful\n",
    "\n",
    "#from google.colab import files\n",
    "\n",
    "import distutils.util\n",
    "import os\n",
    "import subprocess\n",
    "if subprocess.run('nvidia-smi').returncode:\n",
    "  raise RuntimeError(\n",
    "      'Cannot communicate with GPU. '\n",
    "      'Make sure you are using a GPU Colab runtime. '\n",
    "      'Go to the Runtime menu and select Choose runtime type.')\n",
    "\n",
    "# Add an ICD config so that glvnd can pick up the Nvidia EGL driver.\n",
    "# This is usually installed as part of an Nvidia driver package, but the Colab\n",
    "# kernel doesn't install its driver via APT, and as a result the ICD is missing.\n",
    "# (https://github.com/NVIDIA/libglvnd/blob/master/src/EGL/icd_enumeration.md)\n",
    "NVIDIA_ICD_CONFIG_PATH = '/usr/share/glvnd/egl_vendor.d/10_nvidia.json'\n",
    "if not os.path.exists(NVIDIA_ICD_CONFIG_PATH):\n",
    "  with open(NVIDIA_ICD_CONFIG_PATH, 'w') as f:\n",
    "    f.write(\"\"\"{\n",
    "    \"file_format_version\" : \"1.0.0\",\n",
    "    \"ICD\" : {\n",
    "        \"library_path\" : \"libEGL_nvidia.so.0\"\n",
    "    }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "# Configure MuJoCo to use the EGL rendering backend (requires GPU)\n",
    "print('Setting environment variable to use GPU rendering:')\n",
    "%env MUJOCO_GL=egl\n",
    "\n",
    "try:\n",
    "  print('Checking that the installation succeeded:')\n",
    "  import mujoco\n",
    "  mujoco.MjModel.from_xml_string('<mujoco/>')\n",
    "except Exception as e:\n",
    "  raise e from RuntimeError(\n",
    "      'Something went wrong during installation. Check the shell output above '\n",
    "      'for more information.\\n'\n",
    "      'If using a hosted Colab runtime, make sure you enable GPU acceleration '\n",
    "      'by going to the Runtime menu and selecting \"Choose runtime type\".')\n",
    "\n",
    "print('Installation successful.')\n",
    "\n",
    "#@title Import packages for plotting and creating graphics\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "from typing import Callable, NamedTuple, Optional, Union, List\n",
    "\n",
    "# Graphics and plotting.\n",
    "print('Installing mediapy:')\n",
    "!command -v ffmpeg >/dev/null || (apt update && apt install -y ffmpeg)\n",
    "!pip install -q mediapy\n",
    "import mediapy as media\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# More legible printing from numpy.\n",
    "np.set_printoptions(precision=3, suppress=True, linewidth=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DISPLAY=897985abde1b:10.0\n"
     ]
    }
   ],
   "source": [
    "%env DISPLAY=897985abde1b:10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'\\nimport random\\nimport gymnasium as gym\\nenv = gym.make('Hopper', render_mode='rgb_array')\\nenv.reset()\\nn_frames = 120\\nheight = 480\\nwidth = 480\\nframes = []\\n#import mujoco\\nmodel = env.model\\ndata = env.data\\n\\nmujoco.mj_step(model, data)\\nrenderer = mujoco.Renderer(model, height, width)\\nmujoco.mj_resetDataKeyframe(model, data,0)\\n\\njoint_names = ['thigh_joint', 'leg_joint', 'foot_joint']\\njoint_ranges = [model.joint(joint).range for joint in joint_names]\\nfor i in range(n_frames):\\n  while data.time < i/30.0:   \\n    new_action = []\\n        \\n    # ob, reward, terminated, False, {}\\n    obs, reward, terminated, _, _ = env.step(new_action)\\n    #mujoco.mj_step(model, data)\\n  renderer.update_scene(data)\\n  frame = renderer.render()\\n  frames.append(frame)\\nmedia.show_video(frames, fps=30)\\n\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''''\n",
    "import random\n",
    "import gymnasium as gym\n",
    "env = gym.make('Hopper', render_mode='rgb_array')\n",
    "env.reset()\n",
    "n_frames = 120\n",
    "height = 480\n",
    "width = 480\n",
    "frames = []\n",
    "#import mujoco\n",
    "model = env.model\n",
    "data = env.data\n",
    "\n",
    "mujoco.mj_step(model, data)\n",
    "renderer = mujoco.Renderer(model, height, width)\n",
    "mujoco.mj_resetDataKeyframe(model, data,0)\n",
    "\n",
    "joint_names = ['thigh_joint', 'leg_joint', 'foot_joint']\n",
    "joint_ranges = [model.joint(joint).range for joint in joint_names]\n",
    "for i in range(n_frames):\n",
    "  while data.time < i/30.0:   \n",
    "    new_action = []\n",
    "        \n",
    "    # ob, reward, terminated, False, {}\n",
    "    obs, reward, terminated, _, _ = env.step(new_action)\n",
    "    #mujoco.mj_step(model, data)\n",
    "  renderer.update_scene(data)\n",
    "  frame = renderer.render()\n",
    "  frames.append(frame)\n",
    "media.show_video(frames, fps=30)\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ./logs/tqc/PointMaze_Large-v3_16/rl_model_425000_steps.zip\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/research/rl_zoo_jesnk/enjoy_2dmaze.ipynb Cell 5\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfisher-docker/research/rl_zoo_jesnk/enjoy_2dmaze.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m args\u001b[39m.\u001b[39menv_kwargs \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mrender_mode\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mrgb_array\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mcontinuing_task\u001b[39m\u001b[39m'\u001b[39m:\u001b[39mFalse\u001b[39;00m}\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfisher-docker/research/rl_zoo_jesnk/enjoy_2dmaze.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=69'>70</a>\u001b[0m args\u001b[39m.\u001b[39mn_timesteps \u001b[39m=\u001b[39m \u001b[39m3000\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bfisher-docker/research/rl_zoo_jesnk/enjoy_2dmaze.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=70'>71</a>\u001b[0m enjoy(args)\n",
      "\u001b[1;32m/research/rl_zoo_jesnk/enjoy_2dmaze.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bfisher-docker/research/rl_zoo_jesnk/enjoy_2dmaze.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=102'>103</a>\u001b[0m     env_kwargs\u001b[39m.\u001b[39mupdate(args\u001b[39m.\u001b[39menv_kwargs)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bfisher-docker/research/rl_zoo_jesnk/enjoy_2dmaze.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=104'>105</a>\u001b[0m log_dir \u001b[39m=\u001b[39m args\u001b[39m.\u001b[39mreward_log \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mreward_log \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bfisher-docker/research/rl_zoo_jesnk/enjoy_2dmaze.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=106'>107</a>\u001b[0m env \u001b[39m=\u001b[39m create_test_env(\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bfisher-docker/research/rl_zoo_jesnk/enjoy_2dmaze.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=107'>108</a>\u001b[0m     env_name\u001b[39m.\u001b[39;49mgym_id,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bfisher-docker/research/rl_zoo_jesnk/enjoy_2dmaze.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=108'>109</a>\u001b[0m     n_envs\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mn_envs,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bfisher-docker/research/rl_zoo_jesnk/enjoy_2dmaze.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=109'>110</a>\u001b[0m     stats_path\u001b[39m=\u001b[39;49mmaybe_stats_path,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bfisher-docker/research/rl_zoo_jesnk/enjoy_2dmaze.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=110'>111</a>\u001b[0m     seed\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mseed,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bfisher-docker/research/rl_zoo_jesnk/enjoy_2dmaze.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=111'>112</a>\u001b[0m     log_dir\u001b[39m=\u001b[39;49mlog_dir,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bfisher-docker/research/rl_zoo_jesnk/enjoy_2dmaze.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=112'>113</a>\u001b[0m     should_render\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m args\u001b[39m.\u001b[39;49mno_render \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m args\u001b[39m.\u001b[39;49mrender_rgb \u001b[39melse\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m, \u001b[39m# jskang \u001b[39;49;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bfisher-docker/research/rl_zoo_jesnk/enjoy_2dmaze.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=113'>114</a>\u001b[0m     hyperparams\u001b[39m=\u001b[39;49mhyperparams,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bfisher-docker/research/rl_zoo_jesnk/enjoy_2dmaze.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=114'>115</a>\u001b[0m     env_kwargs\u001b[39m=\u001b[39;49menv_kwargs,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bfisher-docker/research/rl_zoo_jesnk/enjoy_2dmaze.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=115'>116</a>\u001b[0m )\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bfisher-docker/research/rl_zoo_jesnk/enjoy_2dmaze.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=117'>118</a>\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(seed\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mseed)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bfisher-docker/research/rl_zoo_jesnk/enjoy_2dmaze.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=118'>119</a>\u001b[0m \u001b[39mif\u001b[39;00m algo \u001b[39min\u001b[39;00m off_policy_algos:\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bfisher-docker/research/rl_zoo_jesnk/enjoy_2dmaze.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=119'>120</a>\u001b[0m     \u001b[39m# Dummy buffer size as we don't need memory to enjoy the trained agent\u001b[39;00m\n",
      "File \u001b[0;32m/research/rl_zoo_jesnk/rl_zoo3/utils.py:248\u001b[0m, in \u001b[0;36mcreate_test_env\u001b[0;34m(env_id, n_envs, stats_path, seed, log_dir, should_render, hyperparams, env_kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_env\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m gym\u001b[39m.\u001b[39mEnv:\n\u001b[1;32m    246\u001b[0m     \u001b[39mreturn\u001b[39;00m spec\u001b[39m.\u001b[39mmake(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 248\u001b[0m env \u001b[39m=\u001b[39m make_vec_env(\n\u001b[1;32m    249\u001b[0m     make_env,\n\u001b[1;32m    250\u001b[0m     n_envs\u001b[39m=\u001b[39;49mn_envs,\n\u001b[1;32m    251\u001b[0m     monitor_dir\u001b[39m=\u001b[39;49mlog_dir,\n\u001b[1;32m    252\u001b[0m     seed\u001b[39m=\u001b[39;49mseed,\n\u001b[1;32m    253\u001b[0m     wrapper_class\u001b[39m=\u001b[39;49menv_wrapper,\n\u001b[1;32m    254\u001b[0m     env_kwargs\u001b[39m=\u001b[39;49menv_kwargs,\n\u001b[1;32m    255\u001b[0m     vec_env_cls\u001b[39m=\u001b[39;49mvec_env_cls,\n\u001b[1;32m    256\u001b[0m     vec_env_kwargs\u001b[39m=\u001b[39;49mvec_env_kwargs,\n\u001b[1;32m    257\u001b[0m )\n\u001b[1;32m    259\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvec_env_wrapper\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m hyperparams\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    260\u001b[0m     vec_env_wrapper \u001b[39m=\u001b[39m get_wrapper_class(hyperparams, \u001b[39m\"\u001b[39m\u001b[39mvec_env_wrapper\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/env2/lib/python3.10/site-packages/stable_baselines3/common/env_util.py:125\u001b[0m, in \u001b[0;36mmake_vec_env\u001b[0;34m(env_id, n_envs, seed, start_index, monitor_dir, wrapper_class, env_kwargs, vec_env_cls, vec_env_kwargs, monitor_kwargs, wrapper_kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39mif\u001b[39;00m vec_env_cls \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    122\u001b[0m     \u001b[39m# Default: use a DummyVecEnv\u001b[39;00m\n\u001b[1;32m    123\u001b[0m     vec_env_cls \u001b[39m=\u001b[39m DummyVecEnv\n\u001b[0;32m--> 125\u001b[0m vec_env \u001b[39m=\u001b[39m vec_env_cls([make_env(i \u001b[39m+\u001b[39;49m start_index) \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(n_envs)], \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mvec_env_kwargs)\n\u001b[1;32m    126\u001b[0m \u001b[39m# Prepare the seeds for the first reset\u001b[39;00m\n\u001b[1;32m    127\u001b[0m vec_env\u001b[39m.\u001b[39mseed(seed)\n",
      "File \u001b[0;32m/env2/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:30\u001b[0m, in \u001b[0;36mDummyVecEnv.__init__\u001b[0;34m(self, env_fns)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, env_fns: List[Callable[[], gym\u001b[39m.\u001b[39mEnv]]):\n\u001b[0;32m---> 30\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvs \u001b[39m=\u001b[39m [_patch_env(fn()) \u001b[39mfor\u001b[39;00m fn \u001b[39min\u001b[39;00m env_fns]\n\u001b[1;32m     31\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mset\u001b[39m([\u001b[39mid\u001b[39m(env\u001b[39m.\u001b[39munwrapped) \u001b[39mfor\u001b[39;00m env \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvs])) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvs):\n\u001b[1;32m     32\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     33\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mYou tried to create multiple environments, but the function to create them returned the same instance \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39minstead of creating different objects. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mPlease read https://github.com/DLR-RM/stable-baselines3/issues/1151 for more information.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     40\u001b[0m         )\n",
      "File \u001b[0;32m/env2/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:30\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, env_fns: List[Callable[[], gym\u001b[39m.\u001b[39mEnv]]):\n\u001b[0;32m---> 30\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvs \u001b[39m=\u001b[39m [_patch_env(fn()) \u001b[39mfor\u001b[39;00m fn \u001b[39min\u001b[39;00m env_fns]\n\u001b[1;32m     31\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mset\u001b[39m([\u001b[39mid\u001b[39m(env\u001b[39m.\u001b[39munwrapped) \u001b[39mfor\u001b[39;00m env \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvs])) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvs):\n\u001b[1;32m     32\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     33\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mYou tried to create multiple environments, but the function to create them returned the same instance \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39minstead of creating different objects. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mPlease read https://github.com/DLR-RM/stable-baselines3/issues/1151 for more information.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     40\u001b[0m         )\n",
      "File \u001b[0;32m/env2/lib/python3.10/site-packages/stable_baselines3/common/env_util.py:98\u001b[0m, in \u001b[0;36mmake_vec_env.<locals>.make_env.<locals>._init\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m         env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(env_id, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39menv_kwargs)\n\u001b[1;32m     97\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 98\u001b[0m     env \u001b[39m=\u001b[39m env_id(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49menv_kwargs)\n\u001b[1;32m     99\u001b[0m     \u001b[39m# Patch to support gym 0.21/0.26 and gymnasium\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     env \u001b[39m=\u001b[39m _patch_env(env)\n",
      "File \u001b[0;32m/research/rl_zoo_jesnk/rl_zoo3/utils.py:246\u001b[0m, in \u001b[0;36mcreate_test_env.<locals>.make_env\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_env\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m gym\u001b[39m.\u001b[39mEnv:\n\u001b[0;32m--> 246\u001b[0m     \u001b[39mreturn\u001b[39;00m spec\u001b[39m.\u001b[39;49mmake(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/env2/lib/python3.10/site-packages/gymnasium/envs/registration.py:130\u001b[0m, in \u001b[0;36mEnvSpec.make\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Env:\n\u001b[1;32m    129\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Calls ``make`` using the environment spec and any keyword arguments.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m     \u001b[39mreturn\u001b[39;00m make(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/env2/lib/python3.10/site-packages/gymnasium/envs/registration.py:802\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    799\u001b[0m     render_mode \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 802\u001b[0m     env \u001b[39m=\u001b[39m env_creator(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49menv_spec_kwargs)\n\u001b[1;32m    803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    804\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    805\u001b[0m         \u001b[39mstr\u001b[39m(e)\u001b[39m.\u001b[39mfind(\u001b[39m\"\u001b[39m\u001b[39mgot an unexpected keyword argument \u001b[39m\u001b[39m'\u001b[39m\u001b[39mrender_mode\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    806\u001b[0m         \u001b[39mand\u001b[39;00m apply_human_rendering\n\u001b[1;32m    807\u001b[0m     ):\n",
      "File \u001b[0;32m/env2/lib/python3.10/site-packages/gymnasium_robotics/envs/maze/point_maze.py:333\u001b[0m, in \u001b[0;36mPointMazeEnv.__init__\u001b[0;34m(self, maze_map, render_mode, reward_type, continuing_task, reset_target, **kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m maze_length \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(maze_map)\n\u001b[1;32m    331\u001b[0m default_camera_config \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mdistance\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m12.5\u001b[39m \u001b[39mif\u001b[39;00m maze_length \u001b[39m>\u001b[39m \u001b[39m8\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m8.8\u001b[39m}\n\u001b[0;32m--> 333\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpoint_env \u001b[39m=\u001b[39m PointEnv(\n\u001b[1;32m    334\u001b[0m     xml_file\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtmp_xml_file_path,\n\u001b[1;32m    335\u001b[0m     render_mode\u001b[39m=\u001b[39;49mrender_mode,\n\u001b[1;32m    336\u001b[0m     default_camera_config\u001b[39m=\u001b[39;49mdefault_camera_config,\n\u001b[1;32m    337\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    338\u001b[0m )\n\u001b[1;32m    339\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model_names \u001b[39m=\u001b[39m MujocoModelNames(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpoint_env\u001b[39m.\u001b[39mmodel)\n\u001b[1;32m    340\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_site_id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model_names\u001b[39m.\u001b[39msite_name2id[\u001b[39m\"\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/env2/lib/python3.10/site-packages/gymnasium_robotics/envs/maze/point.py:39\u001b[0m, in \u001b[0;36mPointEnv.__init__\u001b[0;34m(self, xml_file, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mif\u001b[39;00m xml_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     xml_file \u001b[39m=\u001b[39m path\u001b[39m.\u001b[39mjoin(\n\u001b[1;32m     37\u001b[0m         path\u001b[39m.\u001b[39mdirname(path\u001b[39m.\u001b[39mrealpath(\u001b[39m__file__\u001b[39m)), \u001b[39m\"\u001b[39m\u001b[39m../assets/point/point.xml\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m     )\n\u001b[0;32m---> 39\u001b[0m observation_space \u001b[39m=\u001b[39m spaces\u001b[39m.\u001b[39;49mBox(\n\u001b[1;32m     40\u001b[0m     low\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49mnp\u001b[39m.\u001b[39;49minf, high\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49minf, shape\u001b[39m=\u001b[39;49m(\u001b[39m4\u001b[39;49m,), dtype\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mfloat64\n\u001b[1;32m     41\u001b[0m )\n\u001b[1;32m     42\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[1;32m     43\u001b[0m     model_path\u001b[39m=\u001b[39mxml_file,\n\u001b[1;32m     44\u001b[0m     frame_skip\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     45\u001b[0m     observation_space\u001b[39m=\u001b[39mobservation_space,\n\u001b[1;32m     46\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m     47\u001b[0m )\n",
      "File \u001b[0;32m/env2/lib/python3.10/site-packages/gymnasium/spaces/box.py:109\u001b[0m, in \u001b[0;36mBox.__init__\u001b[0;34m(self, low, high, shape, dtype, seed)\u001b[0m\n\u001b[1;32m    106\u001b[0m _low \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mfull(shape, low, dtype\u001b[39m=\u001b[39m\u001b[39mfloat\u001b[39m) \u001b[39mif\u001b[39;00m is_float_integer(low) \u001b[39melse\u001b[39;00m low\n\u001b[1;32m    107\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbounded_below: NDArray[np\u001b[39m.\u001b[39mbool_] \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39minf \u001b[39m<\u001b[39m _low\n\u001b[0;32m--> 109\u001b[0m _high \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mfull(shape, high, dtype\u001b[39m=\u001b[39m\u001b[39mfloat\u001b[39m) \u001b[39mif\u001b[39;00m is_float_integer(high) \u001b[39melse\u001b[39;00m high\n\u001b[1;32m    110\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbounded_above: NDArray[np\u001b[39m.\u001b[39mbool_] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39minf \u001b[39m>\u001b[39m _high\n\u001b[1;32m    112\u001b[0m low \u001b[39m=\u001b[39m _broadcast(low, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype, shape)\n",
      "File \u001b[0;32m/env2/lib/python3.10/site-packages/gymnasium/spaces/box.py:30\u001b[0m, in \u001b[0;36mis_float_integer\u001b[0;34m(var)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mstr\u001b[39m(np\u001b[39m.\u001b[39mmin(arr))\n\u001b[1;32m     27\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mstr\u001b[39m(arr)\n\u001b[0;32m---> 30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_float_integer\u001b[39m(var: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[1;32m     31\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Checks if a variable is an integer or float.\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39missubdtype(\u001b[39mtype\u001b[39m(var), np\u001b[39m.\u001b[39minteger) \u001b[39mor\u001b[39;00m np\u001b[39m.\u001b[39missubdtype(\u001b[39mtype\u001b[39m(var), np\u001b[39m.\u001b[39mfloating)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--env\", help=\"environment ID\", type=EnvironmentName, default=\"CartPole-v1\")\n",
    "parser.add_argument(\"-f\", \"--folder\", help=\"Log folder\", type=str, default=\"rl-trained-agents\")\n",
    "parser.add_argument(\"--algo\", help=\"RL Algorithm\", default=\"ppo\", type=str, required=False, choices=list(ALGOS.keys()))\n",
    "parser.add_argument(\"-n\", \"--n-timesteps\", help=\"number of timesteps\", default=1000, type=int)\n",
    "parser.add_argument(\"--num-threads\", help=\"Number of threads for PyTorch (-1 to use default)\", default=-1, type=int)\n",
    "parser.add_argument(\"--n-envs\", help=\"number of environments\", default=1, type=int)\n",
    "parser.add_argument(\"--exp-id\", help=\"Experiment ID (default: 0: latest, -1: no exp folder)\", default=0, type=int)\n",
    "parser.add_argument(\"--verbose\", help=\"Verbose mode (0: no output, 1: INFO)\", default=1, type=int)\n",
    "parser.add_argument(\n",
    "    \"--no-render\", action=\"store_true\", default=False, help=\"Do not render the environment (useful for tests)\"\n",
    ")\n",
    "parser.add_argument(\"--deterministic\", action=\"store_true\", default=False, help=\"Use deterministic actions\")\n",
    "parser.add_argument(\"--device\", help=\"PyTorch device to be use (ex: cpu, cuda...)\", default=\"auto\", type=str)\n",
    "parser.add_argument(\n",
    "    \"--load-best\", action=\"store_true\", default=False, help=\"Load best model instead of last model if available\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--load-checkpoint\",\n",
    "    type=int,\n",
    "    help=\"Load checkpoint instead of last model if available, \"\n",
    "    \"you must pass the number of timesteps corresponding to it\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--load-last-checkpoint\",\n",
    "    action=\"store_true\",\n",
    "    default=False,\n",
    "    help=\"Load last checkpoint instead of last model if available\",\n",
    ")\n",
    "parser.add_argument(\"--stochastic\", action=\"store_true\", default=False, help=\"Use stochastic actions\")\n",
    "parser.add_argument(\n",
    "    \"--norm-reward\", action=\"store_true\", default=False, help=\"Normalize reward if applicable (trained with VecNormalize)\"\n",
    ")\n",
    "parser.add_argument(\"--seed\", help=\"Random generator seed\", type=int, default=0)\n",
    "parser.add_argument(\"--reward-log\", help=\"Where to log reward\", default=\"\", type=str)\n",
    "parser.add_argument(\n",
    "    \"--gym-packages\",\n",
    "    type=str,\n",
    "    nargs=\"+\",\n",
    "    default=[],\n",
    "    help=\"Additional external Gym environment package modules to import\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--env-kwargs\", type=str, nargs=\"+\", action=StoreDict, help=\"Optional keyword argument to pass to the env constructor\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--custom-objects\", action=\"store_true\", default=False, help=\"Use custom objects to solve loading issues\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"-P\",\n",
    "    \"--progress\",\n",
    "    action=\"store_true\",\n",
    "    default=False,\n",
    "    help=\"if toggled, display a progress bar using tqdm and rich\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--render_rgb\", action='store_true', default=False, help=\"if toggled, render rgb array\"\n",
    ")\n",
    "#args = parser.parse_args()\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "args.env = EnvironmentName('PointMaze_Large-v3')\n",
    "args.algo = 'tqc'\n",
    "args.load_last_checkpoint = True\n",
    "args.no_render = True\n",
    "args.render_rgb = True\n",
    "args.folder = './logs/'\n",
    "args.exp_id = 16\n",
    "args.env_kwargs = {'render_mode': 'rgb_array','continuing_task':False}\n",
    "args.n_timesteps = 3000\n",
    "args.device = 'cuda'\n",
    "enjoy(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
